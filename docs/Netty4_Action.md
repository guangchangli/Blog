# Netty4 Action

## 网络通信

### 网络五层模型

```
应用层
传输层      四层交换机、四层路由器
网络层      路由器、三层交换机
数据链路层  网桥、以太网交换机、网卡
物理层 		中继器、集线器、双绞线
```

### 数据

- 数据帧

  ```
  Head 18 字节 发送者｜源地址 6 字节 接受者|目标地址 6 字节 数据类型 6 字节
  Data 最短 46字节 最长 1500 字节
  最短 64 字节，最长 1518 字节(超过分片发送)
  ```

- MAC

  ```
  通信数据帧
  目 标  MAC 地址 6B
  发送源 MAC 地址 6B
  TPID 2B
  TCI 2B (含 12 位 VLAN 标识)
  类型 2B
  数据内容 46-1500 字节
  CRC 4B
  ```

- IP

  ```
  32 位二进制 4 段十进制表示
  主机位 + 网络位
  子网掩码 &
  ABCD
  ```

- IP 数据包

  ```
  HEAD 20 - 60 B
  DATA 65515 B
  ```

- ```
  以太网头部｜IP 头部｜TCP[UDP]头部 数据
  ```

- DNS

  ```
  顶级域名 TLD 分为 国家顶级域名 nTLD、通用顶级域名 gTLD、基础结构域名
  域名服务器分为 根域名服务器、顶级域名服务器、本地域名服务器、权限域名服务器
  DNS 使用 TCP|UDP 53 端口，对于每一级域名长度限制是 63 字符，域名总长度不能超过 253 字符
  ```

  

### ARP

```
Ip 找 MAC
广播发出，单播回复
arp -a
```

### Port

```
关联 app 和 网卡
TCP 头部留给存储端口空间 2B 
 0    - 1023  系统保留
1024  - 49152 用户注册
49152 - 65535 随机动态
```

### 建立连接

```
shake_step1:
	client 发送 SYN (syn=1) -> server 进入 SYN_SENT 等待 server 确认
shake_step2:
	server 接受 SYN 确认 client SYN（ack=x+1） 发送 SYN （syn=1）SYN + ACK server 进入 SYN_RECV 状态
shake_step3:
	clent 接收到 server SYN + ACK ，向 server 发送确认包 ACK (sck=y+1) 此包发送完毕，建立起连接
```



### 连接释放

```
wave_step1 
```

### 网络通信原理

```

```

### 浏览器+URL

```
1.DNS 服务器解析域名 -> IP
2.发送 HTTP 请求
3.TCP 设置端口，封装数据
4.IP 封装数据+TCP数据，设置双方IP
5.以太网协议 封装IP数据包，设置双方MAC地址（ARP
6.服务器响应，TCP返回数据
```

### Socket

```
通信双方建立连接后，各自维护一个文件，可以写入内容供对方读取，通信结束关闭文件
```

## IO

```
操作系统内核创建文件描述符（File Descriptor,非负整数）标识 Socket，UNIX 一切皆文件
```

### IO 交互

```
用户进程一次完整的 IO 交互流程分为两个阶段
首先经过内核空间，操作系统处理
接着到达用户空间，交由应用程序处理
```

### 用户态和内核态

```
内核态与用户态是Linux 两级保护机制的两种运行级别
Ring0是留给操作系统代码，设备驱动程序代码使用的，它们工作于系统核心态
Ring3则给普通的用户程序使用，它们工作在用户态
```

``转换``

- 外围设备中断 ring3 -> ring0
- 异常 ring3 -> ring0
- 系统调用 ring3 -> ring0 （主动）

```
对于输入操作，进程 I/O 系统调用后，内核会先看缓冲区中有没有相应的缓存数据，如果没有再到设备中读取
设备 I/O 一般速度较慢，内核缓冲区有数据则直接复制到进程空间
所以，网络输入操作一般包括两个阶段
1.等待网络数据到达网卡，然后将数据读到内核缓冲区
2.从内核缓冲区复制数据，然后拷贝到用户空间
```

``I/O 有 内存 I/O、网络 I/O、磁盘 I/O``

## I/O通信模型

**网络环境下 I/O 简单分为两步，第一步等待，第二部数据迁移 **

**提高 I/O 效率，将等待时间降低，发展了五种   { I/O 模型，阻塞 I/O、非阻塞 I/O、多路复用 I/O、信号驱动 I/O } 同步、异步 I/O**

### 1.阻塞 I/O

```
当用户进程调用了 recvfrom 系统调用
内核就开始 I/O 第一阶段准备数据
很多时候苏剧还没有到达，这个时候内核就要等待数据到来
这个时候用户进程就会被阻塞
当数据准备好时，就会吧数据从内核拷贝到用户内存，返回结果，用户进程解除阻塞状态
```

```
阻塞 I/O 在 I/O 执行两个阶段（等待数据和拷贝数据）都被阻塞,但进程阻塞挂起不消耗 CPU
BIO
```

### 2.非阻塞 I/O

```
当用户进程发出 read 操作时，如果内核中的数据还没有准备好，它并不会阻塞用户进程，而是立刻返回一个 error
从用户进程角度讲，发起一个 read 操作后，并不需要等待，马上就可以得到结果
用户进程判断结果是一个 error 时，他就知道数据还没有准备好，可以再次发送 read 操作
一旦内核数据准备好了，再次收到用户进程系统调用，就马上把数据拷贝到用户内存
```

```
非阻塞 I/O 需要不断的主动询问内核数据准备好了没有
Socket 设置 NON_BLOCK
需要进程轮询（重复调用）,消耗 CPU 资源，适合并发量小不需要及时响应的网络应用开发
```

### 3.多路复用 I/O

```
多个进程的 I/O 可以注册到一个复用器(selector)上
当用户进程调用 selector，selector 会监听所有注册进来的 I/O
如果监听的所有 I/O 在内核缓冲区都没有可读数据，selector 调用进程会被阻塞
当任意一个 I/O 在内核缓冲区有可读数据时，select 调用就会返回
调用进程可以自己或者通知另外的进程（注册进程）再次发读取 I/O，读取内核准备好的数据
多个注册进程注册 I/O 后，只有一个select 调用进程被阻塞
```

```
selector 可以处理多个连接，在处理连接上优于BIO,类似幼儿园教师
对于每一个 socket 一般都设置为非阻塞，但是整个用户的进程其实是一直被阻塞（select函数阻塞），不是socket I/O 阻塞
NIO、nginx(epoll、poll、select)
```

### select、poll、epoll

- ### select

```
select 的核心功能是调用tcp文件系统的poll函数，不停的查询
如果没有想要的数据，主动执行一次调度（防止一直占用cpu），直到有一个连接有想要的消息为止
从这里可以看出select的执行方式基本就是不停的调用poll,直到有需要的消息为止。
缺点：
1、每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大；
2、同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大；
3、select支持的文件描述符数量太小了，默认是1024。
优点：
1、select的可移植性更好，在某些Unix系统上不支持poll()。
2、select对于超时值提供了更好的精度：微秒，而poll是毫秒。
```

- ### poll

```
poll本质上和select没有区别
它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态
如果设备就绪则在设备等待队列中加入一项并继续遍历
如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd
这个过程经历了多次无用的遍历
poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。
缺点：
1、大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义；
2、与select一样，poll返回后，需要轮询pollfd来获取就绪的描述符。
优点：
1、poll() 不要求开发者计算最大文件描述符加一的大小。
2、poll() 在应付大数目的文件描述符的时候速度更快，相比于select。
3、它没有最大连接数的限制，原因是它是基于链表来存储的。
```

- ### epoll

```
epoll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时
返回的不是实际的描述符，而是一个代表就绪描述符数量的值
你只需要去epoll指定的一 个数组中依次取得相应数量的文件描述符即可
这里也使用了内存映射技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。
epoll的优点就是改进了前面所说缺点：
1、支持一个进程打开大数目的socket描述符：
相比select，epoll则没有对FD的限制
它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048
举个例子,在1GB内存的机器上大约是10万左右
具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。
2、IO效率不随FD数目增加而线性下降：
epoll不存在这个问题，它只会对"活跃"的socket进行操作
这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的
那么，只有"活跃"的socket才会主动的去调用 callback函数，其他idle状态socket则不会
在这点上，epoll实现了一个"伪"AIO，因为这时候推动力在os内核
在一些 benchmark中，如果所有的socket基本上都是活跃的，比如一个高速LAN环境，epoll并不比select/poll有什么效率
相 反，如果过多使用epoll_ctl,效率相比还有稍微的下降
但是一旦使用idle connections模拟WAN环境,epoll的效率就远在select/poll之上了。
3、使用mmap加速内核与用户空间的消息传递：
这点实际上涉及到epoll的具体实现了
无论是select,poll还是epoll都需要内核把FD消息通知给用户空间
如何避免不必要的内存拷贝就很重要
在这点上，epoll是通过内核于用户空间mmap同一块内存实现的。
```

- ### 对比

```
select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替
而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替
但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程
虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合
而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间
这就是回调机制带来的性能提升
```

```
select，poll每次调用都要把fd集合从用户态往内核态拷贝一次
并且要把current往设备等待队列中挂一次
而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。
```

|            | select                                | poll                                  | Poll                                                         |
| ---------- | ------------------------------------- | ------------------------------------- | ------------------------------------------------------------ |
| 最大连接数 | 1024（x86）\| 2048 （x64）            | 无上限                                | 无上限                                                       |
| IO效率     | 每次调用进行线性遍历，时间复杂度 O(N) | 每次调用进行线性遍历，时间复杂度 O(N) | 事件通知方式，当fd就绪，调用回调函数，就绪fd放到fdlist里面，epoll_wait 返回就可以拿到就绪的 fd，时间复杂度 O(1） |
| fd拷贝     | 每次select 都拷贝                     | 每次poll都拷贝                        | 调用epoll_ct时拷贝进内核并有内核保存，之后每次epol_wait不拷贝 |

### 4.信号驱动IO模型

```
信号驱动I/O 是指进程预先告知内核，向内核注册一个信号处理函数，然后用户进程返回不阻塞
当内核数据就绪时会发送一个信号给进程，用户进程你在信号处理函数中调用I/O读取数据
实际上I/O内核拷贝到用户进程的过程还是阻塞的
信号驱动I/O并没有实现真正的异步，因为通知到进程之后，依然由进程来完成I/O操作，伪异步
```

### 5.异步I/O

```
用户进程发起 aio_read 操作后，给内核传递与 read 相同的描述符、缓冲区指针、缓冲区大小三个参数以及文件偏移
告诉内核当整个操作完成时，如何通过我们立刻就可以开始去做其他的事
当内核收到一个 aio_read 之后，首先会立刻返回，不会对用户进程产生任何阻塞
内核等待数据准备完成，将数据拷贝到用户内存
当一切都完成之后，内核会给用户进程发送一个信号，告诉 aio_read 操作完成
```

```
工作机制：
告诉内核启动某个操作，并让内核在整个操作完成后通知我们
信号驱动I/O是内核通知我们何时可以启动一个I/O操作，I/O操作由用户自定义的信号函数实现
异步I/O模型是由内核告知我们I/O操作何时完成
JAVA7 AIO,不阻塞，数据一步到位，采用 Procator 模式（异步事件通知）
```

